{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3b09ba",
   "metadata": {},
   "source": [
    "# Muon Tutorial\n",
    "\n",
    "For understanding of the increasingly popular muon optimizer\n",
    "\n",
    "## References\n",
    "\n",
    "https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "https://thinkingmachines.ai/blog/modular-manifolds/\n",
    "\n",
    "https://arxiv.org/abs/2502.16982v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be288c",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**Notation**: \n",
    "- $\\theta \\in \\mathbb{R}^d$ = parameters (model weights)\n",
    "- $g_t \\in \\mathbb{R}^d$ = gradient at step $t$\n",
    "- $\\alpha \\in \\mathbb{R}$ = learning rate (scalar)\n",
    "- $\\beta, \\beta_1, \\beta_2 \\in [0,1]$ = decay rates (scalars)\n",
    "- $\\epsilon \\in \\mathbb{R}$ = numerical stability constant (scalar, typically small)\n",
    "- $\\lambda \\in \\mathbb{R}$ = weight decay coefficient (scalar)\n",
    "\n",
    "### 1. SGD\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot g_t$$\n",
    "\n",
    "**Dimensionality**: $g_t$ same shape as $\\theta$\n",
    "\n",
    "Basic gradient descent. Moves directly opposite to gradient with fixed learning rate\n",
    "\n",
    "### 2. SGD + Momentum\n",
    "\n",
    "$$v_t = \\beta \\cdot v_{t-1} + (1-\\beta) \\cdot g_t \\quad \\text{[build velocity]}$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot v_t \\quad \\text{[move with velocity]}$$\n",
    "\n",
    "**Dimensionality**: $v_t$ same shape as $\\theta$\n",
    "\n",
    "Adds velocity memory. Exponential moving average of gradients. Smooths updates and accelerates in consistent directions\n",
    "\n",
    "### 3. RMSProp\n",
    "\n",
    "$$v_t = \\beta \\cdot v_{t-1} + (1-\\beta) \\cdot g_t^2 \\quad \\text{[scale track]}$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{g_t}{\\sqrt{v_t + \\epsilon}} \\quad \\text{[adaptive scaling]}$$\n",
    "\n",
    "**Dimensionality**: $v_t$ same shape as $\\theta$\n",
    "\n",
    "Adds adaptive scaling per-parameter. \"Scale track\" = exponential moving average of squared gradients (tracks gradient magnitude history). Dividing by $\\sqrt{v_t}$ gives larger steps to params with small gradients, smaller steps to those with large gradients. Per-parameter adaptive rates\n",
    "\n",
    "### 4. Adam\n",
    "\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\quad \\text{[momentum track]}$$\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot g_t^2 \\quad \\text{[scale track]}$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t + \\epsilon}} \\quad \\text{[combine both]}$$\n",
    "\n",
    "**Dimensionality**: $m_t$, $v_t$, $\\theta$ all same shape (element-wise)\n",
    "\n",
    "Adam $\\approx$ \"RMSProp + Momentum\". Combines both velocity AND scale memory\n",
    "\n",
    "### 5. AdamW\n",
    "\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\quad \\text{[momentum track]}$$\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot g_t^2 \\quad \\text{[scale track]}$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t + \\epsilon}} - \\alpha \\cdot \\lambda \\cdot \\theta_{t-1} \\quad \\text{[Adam + weight decay]}$$\n",
    "\n",
    "**Key difference**: Weight decay $(\\lambda \\cdot \\theta_{t-1})$ applied directly to parameters (decoupled), not added to gradients like typical regularization. More effective regularization with Adam's adaptive rates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52a909",
   "metadata": {},
   "source": [
    "#### Example: Why L2 Regularization is Weaker in Adam\n",
    "\n",
    "L2 regularization: $g_t \\leftarrow g_t + \\lambda \\cdot \\theta_{t-1}$\n",
    "\n",
    "Goes through Adam:\n",
    "The regularization term $\\lambda \\cdot \\theta_{t-1}$ enters $m_t$, then gets divided by $\\sqrt{v_t}$:\n",
    "$$\\text{regularization contribution} \\propto \\frac{\\lambda \\cdot \\theta_{t-1}}{\\sqrt{v_t}}$$\n",
    "\n",
    "AdamW decouples weight decay:\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon} - \\alpha \\cdot \\lambda \\cdot \\theta_{t-1}$$\n",
    "\n",
    "Consistent $\\alpha \\cdot \\lambda$ across all parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38fe04",
   "metadata": {},
   "source": [
    "## Muon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b1f85",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muon-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
